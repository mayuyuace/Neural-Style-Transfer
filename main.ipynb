{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"Load an image and convert it to a torch tensor.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "#构建模型中，除了引入经典模型vgg19，还提取了特定层的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    # Image preprocessing\n",
    "    # VGGNet was trained on ImageNet where images are normalized by mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
    "    # We use the same normalization statistics here. 将图片转换为tensor格式，再归一化\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                             std=(0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load content and style images\n",
    "    # Make the style image same size as the content image 加载内容图片和风格图片\n",
    "    content = load_image(config.content, transform, max_size=config.max_size)\n",
    "    style = load_image(config.style, transform, shape=[content.size(2), content.size(3)])\n",
    "    \n",
    "    # Initialize a target image with the content image 目标图片由内容图片复制而来，将目标图片的参数放入优化器\n",
    "    target = content.clone().requires_grad_(True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([target], lr=config.lr, betas=[0.5, 0.999])\n",
    "    vgg = VGGNet().to(device).eval()\n",
    "    \n",
    "    for step in range(config.total_step):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors 提取目标函数、内容函数、风格函数的特定特征\n",
    "        target_features = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features = vgg(style)\n",
    "        \n",
    "        #初始化风格损失和内容损失\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss with target and content images 计算内容损失\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape convolutional feature maps 将全连接层的结果重塑为矩阵格式\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "\n",
    "            # Compute gram matrix 内积得到gram矩阵\n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "            # Compute style loss with target and style images\n",
    "            #计算风格损失\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "        \n",
    "        # Compute total loss, backprop and optimize 计算总损失，进行梯度置零，反向传播，参数更新\n",
    "        loss = content_loss + config.style_weight * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #每经过若干次迭代就输出迭代结果\n",
    "        if (step+1) % config.log_step == 0:\n",
    "            print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "                   .format(step+1, config.total_step, content_loss.item(), style_loss.item()))\n",
    "            \n",
    "        #每经过若干次迭代就输出一个图片结果\n",
    "        if (step+1) % config.sample_step == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().squeeze()\n",
    "            img = denorm(img).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-{}.jpg'.format(step+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(content='jpg/content.jpg', log_step=10, lr=0.003, max_size=400, sample_step=100, style='jpg/style.jpg', style_weight=100, target='jpg/target.jpg', total_step=2000)\n"
     ]
    }
   ],
   "source": [
    "#设置参数\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--content', type=str, default='jpg/content.jpg')\n",
    "parser.add_argument('--style', type=str, default='jpg/style.jpg')\n",
    "parser.add_argument('--target', type=str, default='jpg/target.jpg')\n",
    "parser.add_argument('--max_size', type=int, default=400)\n",
    "parser.add_argument('--total_step', type=int, default=2000)\n",
    "parser.add_argument('--log_step', type=int, default=10)\n",
    "parser.add_argument('--sample_step', type=int, default=100)\n",
    "parser.add_argument('--style_weight', type=float, default=100)\n",
    "parser.add_argument('--lr', type=float, default=0.003)\n",
    "config = parser.parse_known_args()[0]\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/2000], Content Loss: 1.4381, Style Loss: 83.6343\n",
      "Step [20/2000], Content Loss: 2.8423, Style Loss: 68.5622\n",
      "Step [30/2000], Content Loss: 3.7688, Style Loss: 57.9365\n",
      "Step [40/2000], Content Loss: 4.4737, Style Loss: 49.6662\n",
      "Step [50/2000], Content Loss: 5.0545, Style Loss: 43.0076\n",
      "Step [60/2000], Content Loss: 5.5489, Style Loss: 37.5587\n",
      "Step [70/2000], Content Loss: 5.9760, Style Loss: 33.0521\n",
      "Step [80/2000], Content Loss: 6.3534, Style Loss: 29.2967\n",
      "Step [90/2000], Content Loss: 6.6839, Style Loss: 26.1461\n",
      "Step [100/2000], Content Loss: 6.9778, Style Loss: 23.4865\n",
      "Step [110/2000], Content Loss: 7.2411, Style Loss: 21.2264\n",
      "Step [120/2000], Content Loss: 7.4843, Style Loss: 19.2936\n",
      "Step [130/2000], Content Loss: 7.7092, Style Loss: 17.6300\n",
      "Step [140/2000], Content Loss: 7.9169, Style Loss: 16.1899\n",
      "Step [150/2000], Content Loss: 8.1128, Style Loss: 14.9353\n",
      "Step [160/2000], Content Loss: 8.2975, Style Loss: 13.8362\n",
      "Step [170/2000], Content Loss: 8.4696, Style Loss: 12.8668\n",
      "Step [180/2000], Content Loss: 8.6346, Style Loss: 12.0069\n",
      "Step [190/2000], Content Loss: 8.7947, Style Loss: 11.2392\n",
      "Step [200/2000], Content Loss: 8.9483, Style Loss: 10.5504\n",
      "Step [210/2000], Content Loss: 9.0929, Style Loss: 9.9294\n",
      "Step [220/2000], Content Loss: 9.2304, Style Loss: 9.3665\n",
      "Step [230/2000], Content Loss: 9.3602, Style Loss: 8.8546\n",
      "Step [240/2000], Content Loss: 9.4852, Style Loss: 8.3870\n",
      "Step [250/2000], Content Loss: 9.6049, Style Loss: 7.9579\n",
      "Step [260/2000], Content Loss: 9.7159, Style Loss: 7.5632\n",
      "Step [270/2000], Content Loss: 9.8218, Style Loss: 7.1988\n",
      "Step [280/2000], Content Loss: 9.9224, Style Loss: 6.8615\n",
      "Step [290/2000], Content Loss: 10.0174, Style Loss: 6.5481\n",
      "Step [300/2000], Content Loss: 10.1091, Style Loss: 6.2564\n",
      "Step [310/2000], Content Loss: 10.1962, Style Loss: 5.9844\n",
      "Step [320/2000], Content Loss: 10.2816, Style Loss: 5.7304\n",
      "Step [330/2000], Content Loss: 10.3642, Style Loss: 5.4929\n",
      "Step [340/2000], Content Loss: 10.4427, Style Loss: 5.2709\n",
      "Step [350/2000], Content Loss: 10.5188, Style Loss: 5.0630\n",
      "Step [360/2000], Content Loss: 10.5915, Style Loss: 4.8684\n",
      "Step [370/2000], Content Loss: 10.6620, Style Loss: 4.6860\n",
      "Step [380/2000], Content Loss: 10.7312, Style Loss: 4.5150\n",
      "Step [390/2000], Content Loss: 10.7986, Style Loss: 4.3547\n",
      "Step [400/2000], Content Loss: 10.8636, Style Loss: 4.2044\n",
      "Step [410/2000], Content Loss: 10.9255, Style Loss: 4.0634\n",
      "Step [420/2000], Content Loss: 10.9844, Style Loss: 3.9311\n",
      "Step [430/2000], Content Loss: 11.0408, Style Loss: 3.8068\n",
      "Step [440/2000], Content Loss: 11.0961, Style Loss: 3.6898\n",
      "Step [450/2000], Content Loss: 11.1492, Style Loss: 3.5799\n",
      "Step [460/2000], Content Loss: 11.2004, Style Loss: 3.4765\n",
      "Step [470/2000], Content Loss: 11.2496, Style Loss: 3.3791\n",
      "Step [480/2000], Content Loss: 11.2982, Style Loss: 3.2873\n",
      "Step [490/2000], Content Loss: 11.3448, Style Loss: 3.2009\n",
      "Step [500/2000], Content Loss: 11.3908, Style Loss: 3.1193\n",
      "Step [510/2000], Content Loss: 11.4347, Style Loss: 3.0422\n",
      "Step [520/2000], Content Loss: 11.4779, Style Loss: 2.9692\n",
      "Step [530/2000], Content Loss: 11.5206, Style Loss: 2.9001\n",
      "Step [540/2000], Content Loss: 11.5621, Style Loss: 2.8346\n",
      "Step [550/2000], Content Loss: 11.6017, Style Loss: 2.7725\n",
      "Step [560/2000], Content Loss: 11.6408, Style Loss: 2.7133\n",
      "Step [570/2000], Content Loss: 11.6795, Style Loss: 2.6570\n",
      "Step [580/2000], Content Loss: 11.7170, Style Loss: 2.6033\n",
      "Step [590/2000], Content Loss: 11.7533, Style Loss: 2.5521\n",
      "Step [600/2000], Content Loss: 11.7887, Style Loss: 2.5032\n",
      "Step [610/2000], Content Loss: 11.8237, Style Loss: 2.4564\n",
      "Step [620/2000], Content Loss: 11.8569, Style Loss: 2.4115\n",
      "Step [630/2000], Content Loss: 11.8889, Style Loss: 2.3684\n",
      "Step [640/2000], Content Loss: 11.9203, Style Loss: 2.3270\n",
      "Step [650/2000], Content Loss: 11.9505, Style Loss: 2.2872\n",
      "Step [660/2000], Content Loss: 11.9802, Style Loss: 2.2489\n",
      "Step [670/2000], Content Loss: 12.0087, Style Loss: 2.2119\n",
      "Step [680/2000], Content Loss: 12.0371, Style Loss: 2.1762\n",
      "Step [690/2000], Content Loss: 12.0653, Style Loss: 2.1418\n",
      "Step [700/2000], Content Loss: 12.0928, Style Loss: 2.1084\n",
      "Step [710/2000], Content Loss: 12.1199, Style Loss: 2.0761\n",
      "Step [720/2000], Content Loss: 12.1469, Style Loss: 2.0448\n",
      "Step [730/2000], Content Loss: 12.1730, Style Loss: 2.0145\n",
      "Step [740/2000], Content Loss: 12.1975, Style Loss: 1.9851\n",
      "Step [750/2000], Content Loss: 12.2223, Style Loss: 1.9565\n",
      "Step [760/2000], Content Loss: 12.2473, Style Loss: 1.9287\n",
      "Step [770/2000], Content Loss: 12.2719, Style Loss: 1.9017\n",
      "Step [780/2000], Content Loss: 12.2958, Style Loss: 1.8755\n",
      "Step [790/2000], Content Loss: 12.3185, Style Loss: 1.8499\n",
      "Step [800/2000], Content Loss: 12.3413, Style Loss: 1.8250\n",
      "Step [810/2000], Content Loss: 12.3636, Style Loss: 1.8007\n",
      "Step [820/2000], Content Loss: 12.3854, Style Loss: 1.7771\n",
      "Step [830/2000], Content Loss: 12.4061, Style Loss: 1.7540\n",
      "Step [840/2000], Content Loss: 12.4263, Style Loss: 1.7314\n",
      "Step [850/2000], Content Loss: 12.4454, Style Loss: 1.7095\n",
      "Step [860/2000], Content Loss: 12.4641, Style Loss: 1.6880\n",
      "Step [870/2000], Content Loss: 12.4817, Style Loss: 1.6670\n",
      "Step [880/2000], Content Loss: 12.5002, Style Loss: 1.6465\n",
      "Step [890/2000], Content Loss: 12.5185, Style Loss: 1.6264\n",
      "Step [900/2000], Content Loss: 12.5367, Style Loss: 1.6067\n",
      "Step [910/2000], Content Loss: 12.5551, Style Loss: 1.5875\n",
      "Step [920/2000], Content Loss: 12.5729, Style Loss: 1.5687\n",
      "Step [930/2000], Content Loss: 12.5903, Style Loss: 1.5503\n",
      "Step [940/2000], Content Loss: 12.6069, Style Loss: 1.5322\n",
      "Step [950/2000], Content Loss: 12.6228, Style Loss: 1.5145\n",
      "Step [960/2000], Content Loss: 12.6381, Style Loss: 1.4973\n",
      "Step [970/2000], Content Loss: 12.6538, Style Loss: 1.4803\n",
      "Step [980/2000], Content Loss: 12.6686, Style Loss: 1.4637\n",
      "Step [990/2000], Content Loss: 12.6842, Style Loss: 1.4474\n",
      "Step [1000/2000], Content Loss: 12.7001, Style Loss: 1.4313\n",
      "Step [1010/2000], Content Loss: 12.7145, Style Loss: 1.4156\n",
      "Step [1020/2000], Content Loss: 12.7288, Style Loss: 1.4002\n",
      "Step [1030/2000], Content Loss: 12.7434, Style Loss: 1.3851\n",
      "Step [1040/2000], Content Loss: 12.7573, Style Loss: 1.3702\n",
      "Step [1050/2000], Content Loss: 12.7707, Style Loss: 1.3557\n",
      "Step [1060/2000], Content Loss: 12.7835, Style Loss: 1.3413\n",
      "Step [1070/2000], Content Loss: 12.7961, Style Loss: 1.3274\n",
      "Step [1080/2000], Content Loss: 12.8082, Style Loss: 1.3131\n",
      "Step [1090/2000], Content Loss: 12.8199, Style Loss: 1.3003\n",
      "Step [1100/2000], Content Loss: 12.8322, Style Loss: 1.2861\n",
      "Step [1110/2000], Content Loss: 12.8443, Style Loss: 1.2737\n",
      "Step [1120/2000], Content Loss: 12.8571, Style Loss: 1.2601\n",
      "Step [1130/2000], Content Loss: 12.8691, Style Loss: 1.2479\n",
      "Step [1140/2000], Content Loss: 12.8814, Style Loss: 1.2348\n",
      "Step [1150/2000], Content Loss: 12.8926, Style Loss: 1.2234\n",
      "Step [1160/2000], Content Loss: 12.9038, Style Loss: 1.2109\n",
      "Step [1170/2000], Content Loss: 12.9152, Style Loss: 1.1988\n",
      "Step [1180/2000], Content Loss: 12.9248, Style Loss: 1.1877\n",
      "Step [1190/2000], Content Loss: 12.9352, Style Loss: 1.1750\n",
      "Step [1200/2000], Content Loss: 12.9439, Style Loss: 1.1658\n",
      "Step [1210/2000], Content Loss: 12.9553, Style Loss: 1.1521\n",
      "Step [1220/2000], Content Loss: 12.9628, Style Loss: 1.1445\n",
      "Step [1230/2000], Content Loss: 12.9740, Style Loss: 1.1302\n",
      "Step [1240/2000], Content Loss: 12.9821, Style Loss: 1.1235\n",
      "Step [1250/2000], Content Loss: 12.9935, Style Loss: 1.1094\n",
      "Step [1260/2000], Content Loss: 13.0025, Style Loss: 1.1022\n",
      "Step [1270/2000], Content Loss: 13.0123, Style Loss: 1.0898\n",
      "Step [1280/2000], Content Loss: 13.0224, Style Loss: 1.0802\n",
      "Step [1290/2000], Content Loss: 13.0301, Style Loss: 1.0723\n",
      "Step [1300/2000], Content Loss: 13.0433, Style Loss: 1.0592\n",
      "Step [1310/2000], Content Loss: 13.0477, Style Loss: 1.0564\n",
      "Step [1320/2000], Content Loss: 13.0626, Style Loss: 1.0394\n",
      "Step [1330/2000], Content Loss: 13.0673, Style Loss: 1.0388\n",
      "Step [1340/2000], Content Loss: 13.0794, Style Loss: 1.0222\n",
      "Step [1350/2000], Content Loss: 13.0897, Style Loss: 1.0154\n",
      "Step [1360/2000], Content Loss: 13.0955, Style Loss: 1.0084\n",
      "Step [1370/2000], Content Loss: 13.1077, Style Loss: 0.9947\n",
      "Step [1380/2000], Content Loss: 13.1092, Style Loss: 0.9956\n",
      "Step [1390/2000], Content Loss: 13.1233, Style Loss: 0.9771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1400/2000], Content Loss: 13.1277, Style Loss: 0.9790\n",
      "Step [1410/2000], Content Loss: 13.1386, Style Loss: 0.9614\n",
      "Step [1420/2000], Content Loss: 13.1470, Style Loss: 0.9572\n",
      "Step [1430/2000], Content Loss: 13.1515, Style Loss: 0.9510\n",
      "Step [1440/2000], Content Loss: 13.1640, Style Loss: 0.9367\n",
      "Step [1450/2000], Content Loss: 13.1650, Style Loss: 0.9407\n",
      "Step [1460/2000], Content Loss: 13.1781, Style Loss: 0.9213\n",
      "Step [1470/2000], Content Loss: 13.1862, Style Loss: 0.9204\n",
      "Step [1480/2000], Content Loss: 13.1917, Style Loss: 0.9089\n",
      "Step [1490/2000], Content Loss: 13.2028, Style Loss: 0.9006\n",
      "Step [1500/2000], Content Loss: 13.2007, Style Loss: 0.9047\n",
      "Step [1510/2000], Content Loss: 13.2167, Style Loss: 0.8841\n",
      "Step [1520/2000], Content Loss: 13.2201, Style Loss: 0.8897\n",
      "Step [1530/2000], Content Loss: 13.2300, Style Loss: 0.8744\n",
      "Step [1540/2000], Content Loss: 13.2392, Style Loss: 0.8642\n",
      "Step [1550/2000], Content Loss: 13.2358, Style Loss: 0.8748\n",
      "Step [1560/2000], Content Loss: 13.2526, Style Loss: 0.8504\n",
      "Step [1570/2000], Content Loss: 13.2597, Style Loss: 0.8495\n"
     ]
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
